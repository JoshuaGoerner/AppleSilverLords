{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Doc Corpus & chop into TFiDF-BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/data_train_test.csv\")\n",
    "df_test, df_train = df[df[\"set\"]==\"test\"], df[df[\"set\"]==\"train\"]\n",
    "train_review = df_train[\"review\"]\n",
    "train_lbl = df_train[\"sentiment\"]\n",
    "test_review = df_test[\"review\"]\n",
    "test_lbl = df_test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.3s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.8s finished\n"
     ]
    }
   ],
   "source": [
    "# NAIVE-BAYES classification pipeline\n",
    "pl_clf_jobs = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', MultinomialNB()),\n",
    "                       ])\n",
    "\n",
    "# Parameter grid for hyper parameter tuning (of preprocessing)\n",
    "param_grid = [{'tfidf__stop_words' : [None]}]\n",
    "\n",
    "\n",
    "# create grid search\n",
    "gs_clf_jobs = GridSearchCV(pl_clf_jobs,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=10,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True\n",
    "                          )\n",
    "\n",
    "# run grid search\n",
    "_ = gs_clf_jobs.fit(train_review, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = _.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_review, test_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Shit following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be tested\n",
    "- min/max tfidf\n",
    "- stemming\n",
    "- tokenization\n",
    "- stopwordremoval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom stemmer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a custom Porter Stemmer that suits sklearn\n",
    "class PortStem(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(word) for word in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PortStemNoPunctNum(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(word)\n",
    "                for word \n",
    "                in word_tokenize(\n",
    "                doc.translate(\n",
    "                    str.maketrans(string.punctuation + \"0123456789\",' '*len(string.punctuation + \"0123456789\"))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.6s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   23.6s finished\n"
     ]
    }
   ],
   "source": [
    "# NAIVE-BAYES classification pipeline\n",
    "pl_clf_jobs = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', MultinomialNB()),\n",
    "                       ])\n",
    "\n",
    "# Parameter grid for hyper parameter tuning (of preprocessing)\n",
    "param_grid = [{'tfidf__stop_words' : [None, \"english\"]},\n",
    "              {'tfidf__tokenizer': [None, PortStem(), PortStemNoPunctNum()]}\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "# create grid search\n",
    "gs_clf_jobs = GridSearchCV(pl_clf_jobs,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=2,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True\n",
    "                          )\n",
    "\n",
    "# run grid search\n",
    "_ = gs_clf_jobs.fit(train_review, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = _.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_review, test_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:   28.6s finished\n"
     ]
    }
   ],
   "source": [
    "# KNN classification pipeline\n",
    "pl_clf_jobs = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', KNeighborsClassifier()),\n",
    "                       ])\n",
    "\n",
    "# Parameter grid for hyper parameter tuning (of preprocessing)\n",
    "param_grid = [{'tfidf__stop_words' : [None]},\n",
    "              {'tfidf__tokenizer': [None, PortStem(), PortStemNoPunctNum()]},\n",
    "              {'clf__n_neighbors': [190, 191, 192]},\n",
    "              {'clf__metric': ['cosine']}\n",
    "              \n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "# create grid search\n",
    "gs_clf_jobs = GridSearchCV(pl_clf_jobs,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=2,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True\n",
    "                          )\n",
    "\n",
    "# run grid search\n",
    "_ = gs_clf_jobs.fit(train_review, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = _.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76149999999999995"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(test_review, test_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
